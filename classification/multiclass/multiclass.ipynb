{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hodooai\\anaconda3\\envs\\mnist\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA(GPU) 사용 가능 여부 확인\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 데이터 전처리 및 데이터 로더 생성\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # transforms.Grayscale(),\n",
    "])\n",
    "\n",
    "train_imgfolder = ImageFolder(root='C:/Users/hodooai/Downloads/2021_Huons_Train_Line2_Inspected_v2/train', transform=preprocess)\n",
    "val_imgfolder = ImageFolder(root='C:/Users/hodooai/Downloads/2021_Huons_Train_Line2_Inspected_v2/valid', transform=preprocess)\n",
    "test_imgfolder = ImageFolder(root='C:/Users/hodooai/Downloads/2021_Huons_Train_Line2_Inspected_v2/test', transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_imgfolder,batch_size=64,shuffle=True)\n",
    "val_loader = DataLoader(val_imgfolder,batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_imgfolder,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root\n",
    "# fully connected network network\n",
    "# Convolutional Neural Network\n",
    "\n",
    "class CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, 3, 1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 16, 2, 1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(50176, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1, 3, 224, 224)\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224, 224*224//16)  # 784 = 28(width) x 28(height)\n",
    "        self.fc2 = nn.Linear(224*224//16, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #\n",
    "        x = x.view(-1, 224*224)#다른차원의 크기를 자동으로 조정하는 함수//  # m차원 값을 n차원으로 변경 -1 = 아무거나, 784 = 28 x 28 #  PyTorch에서 텐서의 형태를 변경(reshape)하기 위해 사용되는 메서드입니다\n",
    "\n",
    "        \n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "        x = nn.functional.relu(self.fc5(x))\n",
    "        x = nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11_bn as CNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.064342737197876 train_loss: 7.185851573944092\n",
      "9.90731143951416 train_loss: 2.091719627380371\n",
      "12.707985401153564 train_loss: 1.289452075958252\n",
      "15.526219367980957 train_loss: 1.1063954830169678\n",
      "18.429199695587158 train_loss: 1.8473533391952515\n",
      "21.21617102622986 train_loss: 0.9378305673599243\n",
      "24.030972480773926 train_loss: 1.4273343086242676\n",
      "26.843449115753174 train_loss: 2.2281882762908936\n",
      "29.715237617492676 train_loss: 1.231534719467163\n",
      "32.563363790512085 train_loss: 1.8437926769256592\n",
      "35.379690170288086 train_loss: 1.3182889223098755\n",
      "38.22990846633911 train_loss: 1.0407633781433105\n",
      "41.13417863845825 train_loss: 1.2058112621307373\n",
      "43.921844244003296 train_loss: 1.4218957424163818\n",
      "46.75265383720398 train_loss: 1.0268017053604126\n",
      "49.6490592956543 train_loss: 0.9913254380226135\n",
      "52.50289011001587 train_loss: 1.4277052879333496\n",
      "55.42893171310425 train_loss: 2.2016282081604004\n",
      "58.35615921020508 train_loss: 0.8102055191993713\n",
      "61.268059730529785 train_loss: 1.1612221002578735\n",
      "64.18520712852478 train_loss: 1.066524624824524\n",
      "67.0956974029541 train_loss: 0.8726239204406738\n",
      "68.4790244102478 train_loss: 0.9921396970748901\n",
      "89.05983829498291 sec - loss: 1.5686782598495483 / acc: 28.125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 멀티 클래스 - 결과 값은 한개 - 현재 테스크 합이 1 - 현재 테스크\n",
    "# 멀티 라벨 - 결과 값이 여러개  합이 1 이상 일 수 있음\n",
    "# 모델 생성 및 CUDA로 전송\n",
    "m3 = CNN_Model().to(device)\n",
    "\n",
    "# Loss 함수 및 Optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss() # 여기서 확률 1로 변경해줌 떄때로\n",
    "optimizer = optim.Adam(m3.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(1):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for image, label in train_loader:\n",
    "        image, label = image.to(device=device), label.to(device=device)\n",
    "        # print(xb.shape, yb.shape)\n",
    "        pred = m3(image)\n",
    "        loss : torch.Tensor = criterion(pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"{time.time() - start} train_loss: {loss}\")\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():        \n",
    "        for image, labels in val_loader:\n",
    "            image, label = image.to(device=device), label.to(device=device)\n",
    "            pred = m3(image)\n",
    "            # print(pred.data.max(1)[1], labels.data)\n",
    "            acc = pred.data.max(1)[1].eq(labels.data).sum().item() / len(image) * 100\n",
    "            loss = criterion(pred, labels)\n",
    "            val_acc += acc\n",
    "            val_loss+= loss\n",
    "    val_acc /= len(val_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"{time.time() - start} sec - loss: {val_loss} / acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.27933526039124 sec - loss: 1.5686782598495483 / acc: 28.125\n"
     ]
    }
   ],
   "source": [
    "#테스트 코드\n",
    "test_acc = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        image = image.to(device=device)\n",
    "        label = label.to(device=device)\n",
    "\n",
    "        pred = m3(image)\n",
    "        # print(pred.data.max(1)[1], labels.data)\n",
    "        acc = pred.data.max(1)[1].eq(label.data).sum().item() / len(image) * 100\n",
    "        loss = criterion(pred, label)\n",
    "        test_acc += acc\n",
    "        test_loss+= loss\n",
    "print(f\"{time.time() - start} sec - loss: {val_loss} / acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "torch.save(m3.state_dict(), \"D:/test/a.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 불러오기\n",
    "model = CNN_Model()  # 모델을 생성하거나 초기화합니다.\n",
    "model.load_state_dict(torch.load('D:/test/a.pth'))\n",
    "model.eval()  # 모델을 평가 모드로 설정합니다.\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3002.937858104706 sec - loss: 1.5686782598495483 / acc: 28.125\n"
     ]
    }
   ],
   "source": [
    "#테스트 코드\n",
    "test_acc = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        image = image.to(device=device)\n",
    "        label = label.to(device=device)\n",
    "\n",
    "        pred = model(image)\n",
    "        # print(pred.data.max(1)[1], labels.data)\n",
    "        acc = pred.data.max(1)[1].eq(label.data).sum().item() / len(image) * 100\n",
    "        loss = criterion(pred, label)\n",
    "        test_acc += acc\n",
    "        test_loss+= loss\n",
    "print(f\"{time.time() - start} sec - loss: {val_loss} / acc: {val_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
